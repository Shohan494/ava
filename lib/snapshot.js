'use strict';

const fs = require('fs');
const path = require('path');
const zlib = require('zlib');
const md5Hex = require('md5-hex');
const mkdirp = require('mkdirp');
const kathryn = require('kathryn');

const HASH_BYTES = 16; // A md5 hash is 16 bytes long.

// Increment if encoding layout or Kathryn serialization versions change. Previous AVA versions will not be able to
// decode buffers generated by a newer version, so changing this value will require a major version bump of AVA itself.
// The version is encoded as an unsigned 8 bit integer. If it ever reaches 255 it *must* be encoded as a 16 bit integer
// instead.
const VERSION = 1;

const READABLE_HEADER = Buffer.from(`AVA Snapshot v${VERSION}`, 'ascii');
const READABLE_SEPARATOR = Buffer.from('\n\n---\n\n', 'ascii');

function tryRead(file) {
	try {
		return fs.readFileSync(file);
	} catch (err) {
		if (err.code === 'ENOENT') {
			return null;
		}

		throw err;
	}
}

function combineFormatted(map) {
	const buffers = [];
	let byteLength = 0;

	buffers.push(READABLE_HEADER);
	byteLength += READABLE_HEADER.byteLength;

	for (const pair of map) {
		const testTitle = pair[0];
		const entries = pair[1];

		const titleBuffer = Buffer.from(`\n\n# ${testTitle}\n\n`, 'utf8');
		buffers.push(titleBuffer);
		byteLength += titleBuffer.byteLength;

		const last = entries[entries.length - 1];
		for (const entry of entries) {
			buffers.push(entry);
			byteLength += entry.byteLength;

			if (entry !== last) {
				buffers.push(READABLE_SEPARATOR);
				byteLength += READABLE_SEPARATOR.byteLength;
			}
		}
	}

	buffers.push(Buffer.from('\n'));
	byteLength += 1;
	return Buffer.concat(buffers, byteLength);
}

function encode(map) {
	const buffers = [];
	let byteOffset = 0;

	// Allows 65536 tests using snapshots per file.
	const numTests = Buffer.alloc(2);
	numTests.writeUInt16LE(map.size);
	buffers.push(numTests);
	byteOffset += 2;

	const entries = [];
	for (const pair of map) {
		const hash = pair[0];
		const serializedDescriptors = pair[1];

		buffers.push(Buffer.from(hash, 'hex'));
		byteOffset += HASH_BYTES;

		// Allows 65536 snapshot entries per test.
		const numEntries = Buffer.alloc(2);
		numEntries.writeUInt16LE(serializedDescriptors.length, 0);
		buffers.push(numEntries);
		byteOffset += 2;

		for (const value of serializedDescriptors) {
			// Each pointer is 32 bits, restricting the combined, uncompressed
			// snapshot buffer to 4 GiB.
			const start = Buffer.alloc(4);
			const end = Buffer.alloc(4);
			entries.push({start, end, value});

			buffers.push(start, end);
			byteOffset += 8;
		}
	}

	for (const entry of entries) {
		const start = byteOffset;
		const end = byteOffset + entry.value.byteLength;
		entry.start.writeUInt32LE(start, 0);
		entry.end.writeUInt32LE(end, 0);
		buffers.push(entry.value);
		byteOffset = end;
	}

	const combined = Buffer.concat(buffers, byteOffset);
	const compressed = zlib.gzipSync(combined);

	// The decoder matches on this newline byte (0x0A).
	const separator = Buffer.from('\n', 'ascii');

	const versionHeader = Buffer.alloc(1);
	versionHeader.writeUInt8(VERSION);

	return Buffer.concat([
		READABLE_HEADER,
		separator,
		versionHeader,
		compressed
	], READABLE_HEADER.byteLength + separator.byteLength + versionHeader.byteLength + compressed.byteLength);
}

function decode(buffer) {
	// The version starts after the readable header, which is ended by a newline
	// byte (0x0A).
	const versionOffset = buffer.indexOf(0x0A) + 1;
	const version = buffer.readUInt8(versionOffset);
	if (version !== VERSION) {
		// TODO: Ensure this results in a nice test failure.
		throw new Error(`Snapshot version is v${version}, can only handle v${VERSION}`);
	}

	const compressedOffset = versionOffset + 1;
	const decompressed = zlib.gunzipSync(buffer.slice(compressedOffset));
	let byteOffset = 0;

	const entriesByHash = new Map();
	const numHashes = decompressed.readUInt16LE(byteOffset);
	byteOffset += 2;

	for (let i = 0; i < numHashes; i++) {
		const hash = decompressed.toString('hex', byteOffset, byteOffset + HASH_BYTES);
		byteOffset += HASH_BYTES;

		const numEntries = decompressed.readUInt16LE(byteOffset);
		byteOffset += 2;

		const entries = new Array(numEntries);
		for (let j = 0; j < numEntries; j++) {
			const start = decompressed.readUInt32LE(byteOffset);
			byteOffset += 4;
			const end = decompressed.readUInt32LE(byteOffset);
			byteOffset += 4;
			entries[j] = {start, end};
		}

		entriesByHash.set(hash, entries);
	}

	return {buffer: decompressed, entriesByHash};
}

class MissingStub {
	compare() {
		return {pass: false};
	}

	save() {}
}

class RecordAndPass {
	constructor(dir, name, updating) {
		this.dir = dir;
		this.name = name;
		this.updating = updating;

		this.formatted = new Map();
		this.serialized = new Map();
	}

	compare(testTitle, expected) {
		this.record(testTitle, expected);
		return {pass: true};
	}

	record(testTitle, expected) {
		const descriptor = kathryn.describe(expected);

		const formatted = Buffer.from(kathryn.formatDescriptor(descriptor), 'utf8');
		if (this.formatted.has(testTitle)) {
			this.formatted.get(testTitle).push(formatted);
		} else {
			this.formatted.set(testTitle, [formatted]);
		}

		// Record entries using a md5 hash of the title. This ensures the hash is of
		// a fixed length, making the entries easier to encode and decode.
		const hash = md5Hex(testTitle);
		const serialized = kathryn.serialize(descriptor);
		if (this.serialized.has(hash)) {
			this.serialized.get(hash).push(serialized);
		} else {
			this.serialized.set(hash, [serialized]);
		}
	}

	save() {
		const buffer = encode(this.serialized);
		const readableBuffer = combineFormatted(this.formatted);

		mkdirp.sync(this.dir);
		fs.writeFileSync(path.join(this.dir, this.name + '.snap'), buffer);
		fs.writeFileSync(path.join(this.dir, this.name + '.readable.snap'), readableBuffer);
	}
}

class OnlyCompare {
	constructor(buffer, entriesByHash) {
		this.buffer = buffer;
		this.entriesByHash = entriesByHash;
		this.invocationCounts = new Map(Array.from(this.entriesByHash.keys(), hash => [hash, 0]));
	}

	compare(testTitle, expected) {
		const hash = md5Hex(testTitle);
		if (!this.entriesByHash.has(hash)) {
			return {pass: false};
		}

		const count = this.invocationCounts.get(hash);
		this.invocationCounts.set(hash, count + 1);

		const address = this.entriesByHash.get(hash)[count];
		if (!address) {
			return {pass: false};
		}

		expected = kathryn.describe(expected);

		const encodedDescriptor = this.buffer.slice(address.start, address.end);
		const actual = kathryn.deserialize(encodedDescriptor);
		const pass = kathryn.compareDescriptors(actual, expected);

		return {actual, expected, pass};
	}

	save() {}
}

function load(dir, name, updating) {
	if (updating) {
		// When snapshots are being updated, record expected values but pass all
		// comparisons.
		return new RecordAndPass(dir, name, updating);
	}

	const buffer = tryRead(path.join(dir, name + '.snap'));
	if (!buffer) {
		// If a snapshot file does not exist fail all comparisons.
		return new MissingStub();
	}

	// Only perform comparisons, don't record new expected values.
	const decoded = decode(buffer);
	return new OnlyCompare(decoded.buffer, decoded.entriesByHash);
}
exports.load = load;
